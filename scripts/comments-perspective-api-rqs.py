"""Uses Perspective API to classify toxicity."""

import time
from datetime import datetime
from googleapiclient import discovery
import pandas as pd
import utils

PROJECT_PATH = "/home/reed/Projects/learned-toxicity-reddit/reddit-api/"
INPUT_PATH = f"{PROJECT_PATH}data/comments/20pct-users-subset_comments.csv"
OUTPUT_PATH = f"{PROJECT_PATH}data/comments/toxicity-classified-comments.csv"
LOG_PATH = f"{PROJECT_PATH}logs/perspectiveAPI_{datetime.now()}.txt"

API_KEY = "AIzaSyBQqodph6znituli74a07gc95Bq4P7TbAE"

client = discovery.build(
    "commentanalyzer",
    "v1alpha1",
    developerKey=API_KEY,
    discoveryServiceUrl="https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1",
    static_discovery=False,
)


def query_perspective(comments, log_path):
    """Returns a list of binarized toxicity ratings generated by Perspective
    API."""

    start = time.time()

    out_list = []
    for i, comment in enumerate(comments):
        analyze_request = {
            "comment": {"text": comment},
            "requestedAttributes": {"TOXICITY": {}},
        }
        # retry loop
        for j in range(10):
            try:
                response = client.comments().analyze(body=analyze_request).execute()
                # dig down into this crazy dictionary!
                score = response["attributeScores"]["TOXICITY"]["spanScores"][0][
                    "score"
                ]["value"]
                if score >= 0.7:
                    toxic = 1
                else:
                    toxic = 0
                out_list.append(toxic)

                # logging
                print(f"Finished querying comment {i + 1}")
                estimate = utils.estimate_time_remaining(i, len(comments), start)
                print(f"Time remaining: ~{estimate} hours")

                # break the retry loop
                break

            except Exception as e:
                utils.log_to_file(
                    log_path, f"Error: {e} while fetching toxicity of comment {j}\n"
                )
                sleep_time = 2**j  # each retry waits for longer: 1s, 2s, 4s ...
                time.sleep(sleep_time)
                print(f"Waiting {sleep_time} seconds before retrying")

    return out_list


def main():

    data = pd.read_csv(INPUT_PATH)
    comments_list_raw = list(data["text"])
    comments_list_no_emoji = utils.clean_comments(comments_list_raw)
    toxicity_labels = query_perspective(
        comments=comments_list_no_emoji, log_path=LOG_PATH
    )
    out = pd.DataFrame({"toxic": toxicity_labels})
    with open(OUTPUT_PATH, "w") as file:
        out.to_csv(file, header=True, index=False)


if __name__ == "__main__":
    main()
